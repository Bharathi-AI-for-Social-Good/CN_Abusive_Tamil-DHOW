{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK-ToIaiAEZu"
      },
      "outputs": [],
      "source": [
        "!pip install pandas openpyxl requests openai langchain indic-nlp-library"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This metric uses Google's Perspective API to calculate toxicity scores for hate speech and their corresponding counter-narratives across multiple datasets. It appends the scores as new columns, enabling quantitative comparison of toxicity reduction achieved by different approaches."
      ],
      "metadata": {
        "id": "oCYnPkvEAl09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "\n",
        "API_KEY = \"<PERSPECTIVE_API>\"\n",
        "API_URL = f\"https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key={API_KEY}\"\n",
        "\n",
        "def get_toxicity_score(text):\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "    data = {\n",
        "        'comment': {'text': text},\n",
        "        'languages': ['en'],\n",
        "        'requestedAttributes': {'TOXICITY': {}}\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(API_URL, json=data, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            return result['attributeScores']['TOXICITY']['summaryScore']['value']\n",
        "        else:\n",
        "            print(f\"API error: {response.status_code}, {response.text}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Exception: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_excel_file(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "    if 'Hate speech' not in df.columns or 'fact_counter_narratives' not in df.columns:\n",
        "        print(f\"Required columns not found in {file_path}\")\n",
        "        return None\n",
        "\n",
        "    # Add toxicity score columns\n",
        "    hs_toxicities = []\n",
        "    cn_toxicities = []\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        hs = str(row['Hate speech'])\n",
        "        cn = str(row['fact_counter_narratives'])\n",
        "\n",
        "        hs_score = get_toxicity_score(hs)\n",
        "        cn_score = get_toxicity_score(cn)\n",
        "\n",
        "        hs_toxicities.append(hs_score)\n",
        "        cn_toxicities.append(cn_score)\n",
        "\n",
        "        print(f\"{file_path} | Row {i+1} | CN: {cn_score:.3f}\")\n",
        "        time.sleep(1)\n",
        "\n",
        "    df['HS_Toxicity'] = hs_toxicities\n",
        "    df['CN_Toxicity'] = cn_toxicities\n",
        "\n",
        "    return df\n",
        "\n",
        "dataset_files = [\"<datasets_from_all_approaches>\"]\n",
        "\n",
        "# Process and save each with new toxicity columns\n",
        "for file in dataset_files:\n",
        "    if os.path.exists(file):\n",
        "        updated_df = process_excel_file(file)\n",
        "        updated_df = updated_df.dropna(subset=[\"CN_Toxicity\"])\n",
        "        avg_hs_toxicity = updated_df[\"HS_Toxicity\"].mean()\n",
        "        avg_cn_toxicity = updated_df[\"CN_Toxicity\"].mean()\n",
        "        print(f\"Average HS Toxicity: {avg_hs_toxicity:.3f}\")\n",
        "        print(f\"Average CN Toxicity: {avg_cn_toxicity:.3f}\")\n",
        "    else:\n",
        "        print(f\"File not found: {file}\")\n"
      ],
      "metadata": {
        "id": "OQ0W89SAAKj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This metric uses GPT-based evaluation to automatically score counter-narratives on Factuality, Persuasiveness, Informativeness, and Linguistic Quality. The results are appended to the dataset and saved, providing structured metrics for comparative analysis."
      ],
      "metadata": {
        "id": "WhMpMYWABCYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "# Set your OpenAI API key\n",
        "api_key = \"\"\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "eval_prompt_template = \"\"\"\n",
        "நீங்கள் ஒரு மொழி மதிப்பீட்டுக் கருவியாக செயல்படுகிறீர்கள்.\n",
        "தரவுத்தொகுப்பில் உள்ள ஒவ்வொரு எடுத்துக்காட்டையும் பின்வரும் கூறுகளின் அடிப்படையில் 0 முதல் 1 வரை மதிப்பீடு செய்யவும்:\n",
        "\n",
        "உண்மைத்தன்மை (Factuality): எதிர்பேச்சு, வழங்கப்பட்ட 'context' தகவல்களுக்கு ஏற்பவே உள்ளதா என்பதை மதிப்பீடு செய்யவும்.\n",
        "நம்ப வைக்கும் திறன் (Persuasiveness): இந்த எதிர்பேச்சு, வாசகரின் கருத்தை மாற்ற வைக்கும் வகையில் நம்ப வைக்கும் தன்மை உள்ளதா?\n",
        "தகவலளிப்பு (Informativeness): எதிர்பேச்சு எந்த அளவுக்கு பயனுள்ள, தகவலளிக்கும் தன்மை கொண்டது?\n",
        "மொழித்திறன் (Linguistic Quality): எதிர்பேச்சு, இலக்கண ரீதியாக தெளிவாகவும் ஒழுங்காகவும் உள்ளதா?\n",
        "\n",
        "தகவல் (Context):\n",
        "{context}\n",
        "\n",
        "வெறுப்புப் பேச்சு (Hate Speech):\n",
        "{hs}\n",
        "\n",
        "எதிர்பேச்சு (Counter Narrative):\n",
        "{cn}\n",
        "\n",
        "மதிப்பீட்டு முடிவுகளை கீழே உள்ள JSON வடிவத்தில் மட்டும் அளிக்கவும்:\n",
        "{{\n",
        "\"Factuality\": <score>,\n",
        "\"Persuasiveness\": <score>,\n",
        "\"Informativeness\": <score>,\n",
        "\"Linguistic Quality\": <score>\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "def evaluate_cn(hs, cn, context):\n",
        "    prompt = eval_prompt_template.format(context=context, hs=hs, cn=cn)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        temperature=0,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        content = response.choices[0].message.content.strip()\n",
        "        return json.loads(content)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing response: {e}\")\n",
        "        return {\"Factuality\": None, \"Persuasiveness\": None, \"Informativeness\": None, \"Linguistic Quality\": None}\n",
        "\n",
        "df = pd.read_excel(\"<dataset>.xlsx\")\n",
        "\n",
        "# Storage for scores\n",
        "scores = []\n",
        "\n",
        "# Loop through dataset rows\n",
        "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    context = row[\"Context\"]\n",
        "    hs = row[\"Hate speech\"]\n",
        "    cn = row[\"fact_counter_narratives\"]\n",
        "    score = evaluate_cn(hs, cn, context)\n",
        "    scores.append(score)\n",
        "\n",
        "df_scores = pd.DataFrame(scores)\n",
        "df_final = pd.concat([df, df_scores], axis=1)\n",
        "\n",
        "df_final.to_excel(\"metrics.xlsx\", index=False)\n"
      ],
      "metadata": {
        "id": "GkqPgi00Azod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This metric computes BLEU-1 to BLEU-4 scores to evaluate counter-narratives generated by LLMs against human-authored references."
      ],
      "metadata": {
        "id": "ypRO7rKCCCjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "def tokenize_tamil(text):\n",
        "    return indic_tokenize.trivial_tokenize(text.strip(), lang='ta')\n",
        "\n",
        "def compute_bleu(path, ref_column='Counter Narrative from dataset', cand_column='Counter Narrative from LLM'):\n",
        "    df = pd.read_excel(path)\n",
        "\n",
        "    refs = df[ref_column].dropna().astype(str).tolist()\n",
        "    cands = df[cand_column].dropna().astype(str).tolist()\n",
        "\n",
        "    # Tokenize the references and candidates\n",
        "    references = [[ref.split()] for ref in refs]\n",
        "    candidates = [cand.split() for cand in cands]\n",
        "\n",
        "    # Define weights for BLEU-1 to BLEU-4\n",
        "    weights_map = {\n",
        "        1: (1.0,),\n",
        "        2: (0.5, 0.5),\n",
        "        3: (1/3, 1/3, 1/3),\n",
        "        4: (0.25, 0.25, 0.25, 0.25)\n",
        "    }\n",
        "\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    results = {}\n",
        "\n",
        "    # Calculate BLEU-1 to BLEU-4\n",
        "    for n in range(1, 5):\n",
        "        weights = weights_map[n]\n",
        "        score = corpus_bleu(references, candidates, weights=weights, smoothing_function=smoothing)\n",
        "        results[f\"BLEU-{n}\"] = round(score, 4)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "HF18lDx0BUud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = compute_bleu(path='<dataset>.xlsx')\n",
        "print(bleu)"
      ],
      "metadata": {
        "id": "uda-VlZsBX31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This metric computes cosine similarity scores between human-authored and LLM-generated counter-narratives using Sentence-BERT embeddings. It reports mean and maximum similarity, providing a semantic measure of closeness between references and generated texts."
      ],
      "metadata": {
        "id": "1etfz-lcCLsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def compute_similarity_scores(\n",
        "    file_path: str,\n",
        "    reference_column: str = 'Counter Narrative from dataset',\n",
        "    candidate_column: str = 'Counter Narrative from LLM',\n",
        "    model_name: str = 'distiluse-base-multilingual-cased-v2'\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Computes cosine similarity between reference and candidate counter narratives\n",
        "    using Sentence-BERT embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path (str): Path to the Excel file.\n",
        "    - reference_column (str): Column name for reference counter narratives.\n",
        "    - candidate_column (str): Column name for generated counter narratives.\n",
        "    - model_name (str): sBERT model name to load.\n",
        "\n",
        "    Returns:\n",
        "    - dict with aggregated max and mean similarity scores.\n",
        "    \"\"\"\n",
        "    # Load Sentence-BERT model\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "    df = df[[reference_column, candidate_column]].dropna()\n",
        "\n",
        "    # Extract CNs\n",
        "    reference_texts = df[reference_column].tolist()\n",
        "    candidate_texts = df[candidate_column].tolist()\n",
        "\n",
        "    # Encode both lists\n",
        "    ref_embeddings = model.encode(reference_texts, convert_to_tensor=True)\n",
        "    cand_embeddings = model.encode(candidate_texts, convert_to_tensor=True)\n",
        "\n",
        "    # Compute cosine similarity row-wise (pairwise, not full matrix)\n",
        "    similarities = [\n",
        "        cosine_similarity([cand_emb], [ref_emb])[0][0]\n",
        "        for cand_emb, ref_emb in zip(cand_embeddings, ref_embeddings)\n",
        "    ]\n",
        "\n",
        "    similarities = np.array(similarities)\n",
        "\n",
        "    # Compute statistics\n",
        "    mean_similarity = np.mean(similarities)\n",
        "    max_similarity = np.max(similarities)\n",
        "\n",
        "    print(f\"Mean Cosine Similarity: {mean_similarity:.4f}\")\n",
        "    print(f\"Max Cosine Similarity: {max_similarity:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"mean_similarity\": float(mean_similarity),\n",
        "        \"max_similarity\": float(max_similarity)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "8HUMdH2lBdKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = compute_similarity_scores(\"<dataset>.xlsx\")\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "-bdyqylQBlDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph calculates pairwise percentage agreement between human annotators and the LLM on linguistic quality ratings, and visualizes the results as a heatmap. The plot highlights consistency levels across evaluators."
      ],
      "metadata": {
        "id": "2bOY-j7DCSfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_excel(\"metrics.xlsx\")\n",
        "\n",
        "# Only keep the relevant columns\n",
        "cols = [\"LQ_Human1\", \"LQ_Human2\", \"LQ_Human3\", \"Linguistic_Quality_LLM\"]\n",
        "df = df[cols]\n",
        "\n",
        "df = df.applymap(lambda x: str(x).strip().lower())\n",
        "\n",
        "# Compute percentage agreement matrix\n",
        "agreement = pd.DataFrame(index=cols, columns=cols, dtype=float)\n",
        "for c1 in cols:\n",
        "    for c2 in cols:\n",
        "        agreement.loc[c1, c2] = (df[c1] == df[c2]).mean() * 100\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(agreement, annot=True, fmt=\".1f\", cmap=\"YlGnBu\", vmin=75, vmax=100)\n",
        "plt.title(\"Linguistic Quality Agreement (%)\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fMx3xmhgBvwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plots linguistic quality scores from three human annotators and the LLM across prompts, allowing a visual comparison of rating trends and alignment. The line graph highlights variations and consistency in evaluations."
      ],
      "metadata": {
        "id": "eQsB7TwDCYb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_excel(\"metrics-fact_counter_narratives-1-inaia-kaaval.xlsx\")\n",
        "\n",
        "cols = [\"LQ_Human1\", \"LQ_Human2\", \"LQ_Human3\", \"Linguistic_Quality_LLM\"]\n",
        "df = df[cols]\n",
        "df['Prompt'] = range(1, len(df) + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.plot(df['Prompt'], df['LQ_Human1'], marker='o', linestyle='-', color='green', label='Human1')\n",
        "plt.plot(df['Prompt'], df['LQ_Human2'], marker='o', linestyle='-', color='blue', label='Human2')\n",
        "plt.plot(df['Prompt'], df['LQ_Human3'], marker='o', linestyle='-', color='purple', label='Human3')\n",
        "plt.plot(df['Prompt'], df['Linguistic_Quality_LLM'], marker='o', linestyle='-', color='red', label='LLM')\n",
        "\n",
        "# Formatting\n",
        "plt.xticks(df['Prompt'])\n",
        "plt.ylim(-0.1, 1.1)\n",
        "plt.xlabel('Prompt Number')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Linguistic Quality Scores')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CwUMC9NcB5Yd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}