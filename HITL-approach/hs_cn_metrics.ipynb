{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**HS-CN metrics calculated for all datasets**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7cHOU7-W3ZiX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code reads a dataset from an Excel file and counts the occurrences of each category in the 'Category' column.\n",
        "It then calculates and prints the imbalance degree by dividing the maximum count by the minimum, indicating class imbalance."
      ],
      "metadata": {
        "id": "QRIHq0M_3lGF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJSRLS5VRYLv"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"dataset.xlsx\")\n",
        "\n",
        "category_counts = Counter(df['Category'])\n",
        "print(category_counts)\n",
        "imbalance_degree = max(category_counts.values()) / min(category_counts.values())\n",
        "print(imbalance_degree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyV_Fmi8R4jx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpgRMYXaMEQo"
      },
      "source": [
        "Calculate HTER modified and all based on the corrected hs-cn dataset. Using only the dataset that was generated by the LLM and manually corrected by a human."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFyFdjZ_MLWO"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.metrics import edit_distance\n",
        "\n",
        "def compute_hter(hypothesis, reference):\n",
        "    hyp_tokens = str(hypothesis).strip().split()\n",
        "    ref_tokens = str(reference).strip().split()\n",
        "    edits = edit_distance(hyp_tokens, ref_tokens)\n",
        "    return edits / len(ref_tokens) if len(ref_tokens) > 0 else 0\n",
        "\n",
        "df = pd.read_excel(\"dataset-llm-generated.xlsx\")\n",
        "\n",
        "modified_df = df[df[\"Status\"] == \"Modified\"].copy()\n",
        "\n",
        "# HTER - All\n",
        "df[\"HTER\"] = df.apply(\n",
        "    lambda row: compute_hter(row[\"Counter narrative\"], row[\"Final Counter narrative\"]), axis=1\n",
        ")\n",
        "hter_all = df[\"HTER\"].mean()\n",
        "\n",
        "# HTER - modified\n",
        "modified_df[\"HTER\"] = modified_df.apply(\n",
        "    lambda row: compute_hter(row[\"Counter narrative\"], row[\"Final Counter narrative\"]), axis=1\n",
        ")\n",
        "hter_modified = modified_df[\"HTER\"].mean()\n",
        "\n",
        "\n",
        "# Display results\n",
        "print(f\"HTER - All: {hter_all:.4f}\")\n",
        "print(f\"HTER - Modified: {hter_modified:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3-Y0-XhT6Fr"
      },
      "source": [
        "Calculate the following quantitative metrics on the whole dataset\n",
        "\n",
        "\n",
        "*   Repetition rate\n",
        "*   Novelty\n",
        "*   Vocabulary size\n",
        "*   New Vocabulary\n",
        "*   Reused Vocabulary\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrXhvPtjMw5y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "# Utility to tokenize\n",
        "def tokenize(text):\n",
        "    return text.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).split()\n",
        "\n",
        "# Load versions\n",
        "def load_versions(file_paths):\n",
        "    versions = []\n",
        "    for path in file_paths:\n",
        "        df = pd.read_excel(path)\n",
        "        hs = df['Hate speech'].dropna().astype(str).tolist()\n",
        "        cn = df['Counter Narrative from dataset'].dropna().astype(str).tolist()\n",
        "        versions.append({\n",
        "            \"hs\": [tokenize(h) for h in hs],\n",
        "            \"cn\": [tokenize(c) for c in cn]\n",
        "        })\n",
        "    return versions\n",
        "\n",
        "# Repetition Rate\n",
        "def repetition_rate(samples):\n",
        "    total_tokens = 0\n",
        "    repeated_tokens = 0\n",
        "\n",
        "    for tokens in samples:\n",
        "        if not tokens:\n",
        "            continue\n",
        "        token_counts = Counter(tokens)\n",
        "        total_tokens += len(tokens)\n",
        "        repeated_tokens += sum(count for tok, count in token_counts.items() if count > 1)\n",
        "\n",
        "    if total_tokens == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return (repeated_tokens / total_tokens) * 100\n",
        "\n",
        "# Novelty: how many n-grams are new vs reference\n",
        "def novelty(current, reference):\n",
        "    current_ngrams = set(ng for sent in current for ng in zip(sent, sent[1:]))\n",
        "    reference_ngrams = set(ng for sent in reference for ng in zip(sent, sent[1:]))\n",
        "    novel = current_ngrams - reference_ngrams\n",
        "    return len(novel) / len(current_ngrams) if current_ngrams else 0\n",
        "\n",
        "def jaccard_similarity(current, reference):\n",
        "    current_ngrams = set(ng for sent in current for ng in zip(sent, sent[1:]))\n",
        "    reference_ngrams = set(ng for sent in reference for ng in zip(sent, sent[1:]))\n",
        "\n",
        "    intersection = current_ngrams & reference_ngrams\n",
        "    union = current_ngrams | reference_ngrams\n",
        "\n",
        "    return len(intersection) / len(union) if union else 0\n",
        "\n",
        "\n",
        "# Vocab size and overlap\n",
        "def vocab_stats(current, reference):\n",
        "    current_vocab = set(tok for sent in current for tok in sent)\n",
        "    reference_vocab = set(tok for sent in reference for tok in sent)\n",
        "    new = current_vocab - reference_vocab\n",
        "    reused = current_vocab & reference_vocab\n",
        "    return {\n",
        "        \"vocab_size\": len(current_vocab),\n",
        "        \"new_vocab\": len(new),\n",
        "        \"reused_vocab\": len(reused)\n",
        "    }\n",
        "\n",
        "\n",
        "file_paths = [\n",
        "    \"dataset_v1\",\n",
        "    \"dataset_v2\",\n",
        "    \"dataset_v3\"\n",
        "]\n",
        "\n",
        "versions = load_versions(file_paths)\n",
        "\n",
        "for i in range(1, len(versions)):\n",
        "    curr = versions[i]\n",
        "    prev = versions[i - 1]\n",
        "    cumulative = {\n",
        "        \"hs\": sum([v[\"hs\"] for v in versions[:i]], []),\n",
        "        \"cn\": sum([v[\"cn\"] for v in versions[:i]], [])\n",
        "    }\n",
        "\n",
        "    print(f\"\\n--- Comparing Version V{i+1} ---\")\n",
        "\n",
        "    # Repetition Rate\n",
        "    print(f\"Repetition Rate (HS): {repetition_rate(curr['hs']):.4f}\")\n",
        "    print(f\"Repetition Rate (CN): {repetition_rate(curr['cn']):.4f}\")\n",
        "\n",
        "    # Novelty\n",
        "    print(f\"Novelty HS vs V1: {1 - jaccard_similarity(curr['hs'], versions[0]['hs']):.4f}\")\n",
        "    print(f\"Novelty CN vs V1: {1 - jaccard_similarity(curr['cn'], versions[0]['cn']):.4f}\")\n",
        "    print(f\"Novelty HS vs cumulative: {1 - jaccard_similarity(curr['hs'], cumulative['hs']):.4f}\")\n",
        "    print(f\"Novelty CN vs cumulative: {1 - jaccard_similarity(curr['cn'], cumulative['cn']):.4f}\")\n",
        "    print(f\"Novelty HS vs Vi-1: {1 - jaccard_similarity(curr['hs'], prev['hs']):.4f}\")\n",
        "    print(f\"Novelty CN vs Vi-1: {1 - jaccard_similarity(curr['cn'], prev['cn']):.4f}\")\n",
        "\n",
        "    # Vocab\n",
        "    vocab = vocab_stats(curr['cn'], prev['cn'])\n",
        "    print(f\"CN Vocab Size: {vocab['vocab_size']}\")\n",
        "    print(f\"New Vocab (vs previous): {vocab['new_vocab']}\")\n",
        "    print(f\"Reused Vocab (vs previous): {vocab['reused_vocab']}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDrO-pU6Ktdj"
      },
      "source": [
        "**Data Analytics:** <br/>\n",
        "Word Cloud on Hate speech and Counter Narratives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRxKY_iDWFF5"
      },
      "outputs": [],
      "source": [
        "pip install pandas matplotlib wordcloud"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code processes a the hate speech counter-narrative dataset to extract meaningful words from hate speech and counter-narratives using basic preprocessing and SpaCy's Tamil tokenizer.\n",
        "\n",
        "It then generates separate word clouds for each category based on word frequency, visualizing the most prominent terms."
      ],
      "metadata": {
        "id": "G69YnNCO46Cd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLenwE6-K6zT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from spacy.lang.ta import Tamil\n",
        "\n",
        "# Word cloud for the seed dataset\n",
        "df = pd.read_csv('dataset.csv', encoding='UTF-8-SIG')\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "def is_valid_tamil_word(word):\n",
        "    return (\n",
        "        len(word) > 4 and\n",
        "        all('\\u0B80' <= char <= '\\u0BFF' for char in word) and\n",
        "        not re.match(r'^[a-zA-Z0-9]+$', word)  # exclude English/alphanumeric if any\n",
        "    )\n",
        "\n",
        "df['Hate speech'] = df['Hate speech'].apply(preprocess)\n",
        "df['Counter Narrative from dataset'] = df['Counter Narrative from dataset'].apply(preprocess)\n",
        "\n",
        "# Combine all hate speech and counter narrative texts\n",
        "hate_speech_text = \" \".join(df['Hate speech'].dropna())\n",
        "counter_speech_text = \" \".join(df['Counter Narrative from dataset'].dropna())\n",
        "\n",
        "tamil_nlp = Tamil()\n",
        "hate_tamil_doc = tamil_nlp(hate_speech_text)\n",
        "hate_tamil_tokens = [token.text for token in hate_tamil_doc if is_valid_tamil_word(token.text)]\n",
        "hate_tamil_tokens_counter = Counter(hate_tamil_tokens)\n",
        "\n",
        "counter_tamil_doc = tamil_nlp(counter_speech_text)\n",
        "counter_tamil_tokens = [token.text for token in counter_tamil_doc  if is_valid_tamil_word(token.text)]\n",
        "counter_tamil_tokens_counter = Counter(counter_tamil_tokens)\n",
        "\n",
        "\n",
        "def generate_wordcloud(text, title):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white', font_path='NotoSansTamil_CondensedRegular.ttf').generate_from_frequencies(text)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.tight_layout(pad=0)\n",
        "    plt.show()\n",
        "\n",
        "# Generate each word cloud\n",
        "generate_wordcloud(hate_tamil_tokens_counter, \"Word Cloud - Hate Speech\")\n",
        "generate_wordcloud(counter_tamil_tokens_counter, \"Word Cloud - Counter Narratives\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhu2_CR3-qxX"
      },
      "source": [
        "The below code analyzes the text length distribution of Tamil hate speech and counter-narrative samples by plotting histograms using Seaborn.\n",
        "\n",
        "It also visualizes how many characters are typically used in each category, helping to compare their verbosity and structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7mkL_txMOwd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"tamil-hs-cn-seed-data.csv\")  # Replace with your file path\n",
        "\n",
        "# Clean and prepare data\n",
        "df = df.dropna(subset=['Hate speech', 'Counter Narrative from dataset'])\n",
        "df['Hate_speech_length'] = df['Hate speech'].apply(lambda x: len(str(x)))\n",
        "df['Counter_Narrative_length'] = df['Counter Narrative from dataset'].apply(lambda x: len(str(x)))\n",
        "\n",
        "# Set Seaborn theme for a professional look\n",
        "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "sns.histplot(df['Hate_speech_length'], bins=30, kde=False, color='#fb7f64', label='Hate Speech', alpha=0.7)\n",
        "sns.histplot(df['Counter_Narrative_length'], bins=30, kde=False, color='#4fcafc', label='Counter Narrative', alpha=0.7)\n",
        "\n",
        "# Titles and labels\n",
        "plt.title('Distribution of Text Length in Hate Speech and Counter Narratives', fontsize=16, pad=20)\n",
        "plt.xlabel('Number of characters per text', fontsize=12)\n",
        "plt.ylabel('Number of text observations', fontsize=12)\n",
        "plt.xticks(fontsize=11)\n",
        "plt.xticks(range(0, 900, 100))\n",
        "plt.yticks(fontsize=11)\n",
        "plt.grid(axis='y', linestyle='--', linewidth=0.7, alpha=0.7)\n",
        "\n",
        "# Legend\n",
        "plt.legend(title='Category', title_fontsize=12, fontsize=11)\n",
        "\n",
        "# Layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmJXDg2dBiLl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
